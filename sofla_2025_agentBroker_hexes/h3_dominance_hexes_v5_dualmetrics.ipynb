{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3953bee8",
   "metadata": {},
   "source": [
    "\n",
    "# H3 Dominance Hexes — Dual Metrics (Deals & Price Volume)\n",
    "\n",
    "This notebook builds **H3 hexagon aggregations** for **agents** and **brokerages** and exports GeoJSON files for use in Mapbox.\n",
    "It **always includes both metrics** (deal count and price volume) in every output file, so the front-end can display both\n",
    "regardless of which metric toggle is selected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3813210",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Configuration ====\n",
    "# Path to input table. Supported: CSV or Parquet. Must include columns:\n",
    "# latitude, longitude, price, Agent_Name, Brokerage_Firm\n",
    "INPUT_PATH = \"Adam F. - 2025 Miami Brokerage and Broker Rankings - Agents_Brokers.csv\"   # <-- change to your file name (CSV or Parquet)\n",
    "OUTPUT_PREFIX = \".\"        # folder to write geojsons\n",
    "\n",
    "# Hex resolutions to export\n",
    "RES_LIST = [7, 8, 9]\n",
    "\n",
    "# Minimum deals per hex to include\n",
    "MIN_SAMPLE = 10\n",
    "\n",
    "# Optional: restrict to Miami-Dade bbox (lon/lat): (minx, miny, maxx, maxy)\n",
    "# Set to None to skip filtering\n",
    "MIAMI_DADE_BBOX = (-80.9, 25.1, -80.0, 26.1)  # rough bounds; adjust if needed\n",
    "\n",
    "# If your input uses different column names, map them here\n",
    "COLS = {\n",
    "    \"lat\": \"latitude\",\n",
    "    \"lon\": \"longitude\",\n",
    "    \"price\": \"price\",\n",
    "    \"agent\": \"Agent_Name\",\n",
    "    \"broker\": \"Brokerage_Firm\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6871f7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "\n",
    "# h3 v4+\n",
    "import h3\n",
    "\n",
    "def _to_py_num(x):\n",
    "    if isinstance(x, (np.integer,)):\n",
    "        return int(x)\n",
    "    if isinstance(x, (np.floating,)):\n",
    "        return float(x)\n",
    "    return x\n",
    "\n",
    "def _in_bbox(lon, lat, bbox):\n",
    "    minx, miny, maxx, maxy = bbox\n",
    "    return (lon >= minx) & (lon <= maxx) & (lat >= miny) & (lat <= maxy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6315013f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rows: 26844\n",
      "    latitude  longitude      price             Agent_Name  \\\n",
      "0  25.846485 -80.130144  122125121      ZACHARY VICHINSKY   \n",
      "1  25.846485 -80.130144  122125121  THE JILLS ZEDER GROUP   \n",
      "2  25.848982 -80.126524   74250000   NELSON GONZALEZ TEAM   \n",
      "\n",
      "                Brokerage_Firm  \n",
      "0  BESPOKE REAL ESTATE FLORIDA  \n",
      "1       COLDWELL BANKER REALTY  \n",
      "2              BHHS EWM REALTY  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== Load & clean ====\n",
    "p = Path(INPUT_PATH)\n",
    "if not p.exists():\n",
    "    raise FileNotFoundError(f\"Input file not found: {INPUT_PATH}\")\n",
    "\n",
    "if p.suffix.lower() in (\".parquet\", \".pq\"):\n",
    "    df = pd.read_parquet(p)\n",
    "else:\n",
    "    df = pd.read_csv(p)\n",
    "\n",
    "# Normalize expected columns\n",
    "df = df.rename(columns={\n",
    "    COLS[\"lat\"]: \"latitude\",\n",
    "    COLS[\"lon\"]: \"longitude\",\n",
    "    COLS[\"price\"]: \"price\",\n",
    "    COLS[\"agent\"]: \"Agent_Name\",\n",
    "    COLS[\"broker\"]: \"Brokerage_Firm\",\n",
    "})\n",
    "\n",
    "# Basic cleaning\n",
    "df = df.dropna(subset=[\"latitude\",\"longitude\",\"price\"])\n",
    "df[\"latitude\"] = pd.to_numeric(df[\"latitude\"], errors=\"coerce\")\n",
    "df[\"longitude\"] = pd.to_numeric(df[\"longitude\"], errors=\"coerce\")\n",
    "df[\"price\"] = pd.to_numeric(df[\"price\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "# Optional geographic clip\n",
    "if MIAMI_DADE_BBOX is not None:\n",
    "    mask = _in_bbox(df[\"longitude\"].values, df[\"latitude\"].values, MIAMI_DADE_BBOX)\n",
    "    df = df.loc[mask].copy()\n",
    "\n",
    "# Standardize string columns\n",
    "for c in [\"Agent_Name\",\"Brokerage_Firm\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "print(\"Loaded rows:\", len(df))\n",
    "print(df[[\"latitude\",\"longitude\",\"price\",\"Agent_Name\",\"Brokerage_Firm\"]].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8510e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hex(df_in: pd.DataFrame, res: int, field: str, min_sample: int = 10):\n",
    "    \"\"\"\n",
    "    Build a FeatureCollection of H3 hexes with both metrics (count & volume) per name.\n",
    "    field: 'Agent_Name' or 'Brokerage_Firm'\n",
    "    \"\"\"\n",
    "    df = df_in.dropna(subset=[\"latitude\",\"longitude\"]).copy()\n",
    "    df[\"h3\"] = df.apply(\n",
    "        lambda r: h3.latlng_to_cell(float(r[\"latitude\"]), float(r[\"longitude\"]), int(res)),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    features = []\n",
    "    for h, g in df.groupby(\"h3\", sort=False):\n",
    "        n = len(g)\n",
    "        if n < min_sample:\n",
    "            continue\n",
    "\n",
    "        # aggregate counts & volume per name\n",
    "        agg = g.groupby(field, dropna=False)[\"price\"].agg(count=\"count\", sum=\"sum\")\n",
    "\n",
    "        # Deterministic order:\n",
    "        #   count desc → volume desc → name A→Z (case-insensitive)\n",
    "        df_rank = agg.copy()\n",
    "        df_rank[\"name\"] = df_rank.index.astype(str)\n",
    "        df_rank[\"name_lower\"] = df_rank[\"name\"].str.lower()\n",
    "        df_rank = df_rank.sort_values(\n",
    "            by=[\"count\", \"sum\", \"name_lower\", \"name\"],\n",
    "            ascending=[False, False, True, True],\n",
    "        )\n",
    "\n",
    "        top3 = df_rank.head(3)\n",
    "        top3_names  = top3[\"name\"].tolist()\n",
    "        top3_counts = [int(v) for v in top3[\"count\"].tolist()]\n",
    "        top3_volume = [float(v) for v in top3[\"sum\"].tolist()]\n",
    "\n",
    "        total_price = float(g[\"price\"].sum())\n",
    "        top_count   = int(top3[\"count\"].iloc[0]) if not top3.empty else 0\n",
    "        top_name    = top3_names[0] if top3_names else \"\"\n",
    "\n",
    "        props = {\n",
    "            \"h3\": h,\n",
    "            \"sample_size\": int(n),\n",
    "            \"sum_price\": total_price,\n",
    "            f\"{field}_top3_names\": json.dumps(top3_names, ensure_ascii=False),\n",
    "            f\"{field}_top3_counts\": json.dumps(top3_counts),\n",
    "            f\"{field}_top3_volume\": json.dumps(top3_volume),\n",
    "            # used by the map’s color ramp/label:\n",
    "            f\"top_{field}_share\": float(top_count / n) if n else 0.0,\n",
    "            f\"top_{field}_name\": top_name,\n",
    "        }\n",
    "\n",
    "\n",
    "        try:\n",
    "            # v3/v4 that support positional geo_json flag\n",
    "            boundary = h3.cell_to_boundary(h, True)  # returns [ [lon, lat], ... ]\n",
    "        except TypeError:\n",
    "            # older signature: returns [(lat, lon), ...] — convert to GeoJSON order\n",
    "            coords = h3.cell_to_boundary(h)\n",
    "            boundary = [[lng, lat] for lat, lng in coords]\n",
    "\n",
    "        features.append({\n",
    "            \"type\": \"Feature\",\n",
    "            \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [boundary]},\n",
    "            \"properties\": props,\n",
    "        })\n",
    "\n",
    "    return {\"type\": \"FeatureCollection\", \"features\": features}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9546baff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building res=7…\n",
      "Building res=8…\n",
      "Building res=9…\n",
      "Wrote:\n",
      "  - hex_agent_count_res7.geojson\n",
      "  - hex_agent_volume_res7.geojson\n",
      "  - hex_broker_count_res7.geojson\n",
      "  - hex_broker_volume_res7.geojson\n",
      "  - hex_agent_count_res8.geojson\n",
      "  - hex_agent_volume_res8.geojson\n",
      "  - hex_broker_count_res8.geojson\n",
      "  - hex_broker_volume_res8.geojson\n",
      "  - hex_agent_count_res9.geojson\n",
      "  - hex_agent_volume_res9.geojson\n",
      "  - hex_broker_count_res9.geojson\n",
      "  - hex_broker_volume_res9.geojson\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== Export ====\n",
    "Path(OUTPUT_PREFIX).mkdir(parents=True, exist_ok=True)\n",
    "out_files = []\n",
    "\n",
    "for res in RES_LIST:\n",
    "    print(f\"Building res={res}…\")\n",
    "\n",
    "    for field in [\"Agent_Name\",\"Brokerage_Firm\"]:\n",
    "        fc = compute_hex(df, res=res, field=field, min_sample=MIN_SAMPLE)\n",
    "        mode = \"agent\" if field == \"Agent_Name\" else \"broker\"\n",
    "\n",
    "        # Write two files per mode (count & volume) with identical payload (both metrics included)\n",
    "        for metric in [\"count\",\"volume\"]:\n",
    "            fname = Path(OUTPUT_PREFIX) / f\"hex_{mode}_{metric}_res{res}.geojson\"\n",
    "            fname.write_text(json.dumps(fc, ensure_ascii=False))\n",
    "            out_files.append(str(fname))\n",
    "\n",
    "print(\"Wrote:\")\n",
    "for f in out_files:\n",
    "    print(\"  -\", f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10716eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample file: hex_agent_count_res7.geojson\n",
      "Features: 259\n",
      "Example names: ['THE JILLS ZEDER GROUP', 'KENLEY CERVERA', 'VANESA CARPIGNANO']\n",
      "Example counts: [10, 9, 8]\n",
      "Example volume: [215176084.0, 15187000.0, 17080000.0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== Quick sanity check on one file ====\n",
    "import json\n",
    "sample_file = out_files[0] if out_files else None\n",
    "print(\"Sample file:\", sample_file)\n",
    "if sample_file:\n",
    "    gj = json.loads(Path(sample_file).read_text())\n",
    "    print(\"Features:\", len(gj.get(\"features\", [])))\n",
    "    if gj.get(\"features\"):\n",
    "        p = gj[\"features\"][0][\"properties\"]\n",
    "        names = json.loads(p.get(\"Agent_Name_top3_names\") or p.get(\"Brokerage_Firm_top3_names\") or \"[]\")\n",
    "        counts = json.loads(p.get(\"Agent_Name_top3_counts\") or p.get(\"Brokerage_Firm_top3_counts\") or \"[]\")\n",
    "        volume = json.loads(p.get(\"Agent_Name_top3_volume\") or p.get(\"Brokerage_Firm_top3_volume\") or \"[]\")\n",
    "        print(\"Example names:\", names)\n",
    "        print(\"Example counts:\", counts)\n",
    "        print(\"Example volume:\", volume[:3])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
