{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1906c6b-e591-4491-b83c-18ad3fdc7bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 01:14:00 | INFO | API queries_quota: 60\n"
     ]
    }
   ],
   "source": [
    "# ---- Load Google Maps API key: your %store approach, with env fallback ----\n",
    "try:\n",
    "    get_ipython().run_line_magic(\"store\", \"-r google_maps_API_Key\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import googlemaps\n",
    "\n",
    "if 'google_maps_API_Key' not in globals() or not google_maps_API_Key:\n",
    "    google_maps_API_Key = os.getenv(\"GOOGLE_MAPS_API_KEY\", \"\")\n",
    "\n",
    "if not google_maps_API_Key:\n",
    "    raise ValueError(\"No Google Maps API key found. Set `%store google_maps_API_Key` or env var GOOGLE_MAPS_API_KEY.\")\n",
    "\n",
    "google_maps_API_Key = googlemaps.Client(key=google_maps_API_Key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52712a48-417d-4e40-837f-0674b576dd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 01:34:43 | INFO | API queries_quota: 60\n",
      "2025-09-25 01:34:43 | INFO | Deleted old map_data.geojson\n",
      "2025-09-25 01:34:44 | INFO | Unique addresses to resolve (normalized): 2635\n",
      "Geocoding: 100%|█████████████████████| 2635/2635 [00:00<00:00, 1526728.97addr/s]\n",
      "2025-09-25 01:34:44 | INFO | Wrote 2327 ungeocoded rows to ungeocoded.csv\n",
      "2025-09-25 01:34:44 | INFO | Created 5,640 records\n",
      "2025-09-25 01:34:44 | INFO | Wrote 5640 rows to map_data.geojson (geometries may be null)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Build a GeoJSON of ALL South Florida transactions from CSVs (Miami-Dade, Palm Beach, Broward)\n",
    "— preserving every valid row (no date or doc-type filtering) — and enriching\n",
    "DEED rows by bundling the matching MORTGAGE (Lender + Loan Amount) while\n",
    "keeping mortgage rows in the dataset. Geocoding is cached, Florida-biased, and\n",
    "address-normalized. Rows that fail geocoding remain with geometry=null. Also\n",
    "exports an `ungeocoded.csv` triage file. Includes a tqdm progress bar.\n",
    "\n",
    "Usage (script):\n",
    "  pip install geopandas googlemaps tqdm shapely\n",
    "  export GOOGLE_MAPS_API_KEY=...  # or use Jupyter %store google_maps_API_Key\n",
    "  python transactions_geojson_from_csv.py\n",
    "\n",
    "Outputs:\n",
    "  - map_data.geojson\n",
    "  - geocode_cache.json (on-disk cache)\n",
    "  - ungeocoded.csv (rows missing lat/lon)\n",
    "\n",
    "Notes:\n",
    "  - Never drops rows for dates, doc types, or geocode failures.\n",
    "  - \"Bundling\" = add Lender/Loan Amount onto matching DEED rows; mortgages stay too.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "from typing import Dict, Optional, Tuple\n",
    "\n",
    "import geopandas as gpd\n",
    "import googlemaps\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Google Maps API key (works in scripts AND notebooks)\n",
    "# -----------------------------------------------------------------------------\n",
    "GOOGLE_MAPS_API_KEY = os.getenv(\"GOOGLE_MAPS_API_KEY\", \"\")\n",
    "try:\n",
    "    # If running in a notebook and the user stored `%store google_maps_API_Key`\n",
    "    ip = get_ipython()  # type: ignore[name-defined]\n",
    "    try:\n",
    "        ip.run_line_magic(\"store\", \"-r google_maps_API_Key\")  # loads into local namespace if present\n",
    "        if 'google_maps_API_Key' in globals() and google_maps_API_Key and not GOOGLE_MAPS_API_KEY:  # noqa: F821\n",
    "            GOOGLE_MAPS_API_KEY = google_maps_API_Key  # noqa: F821\n",
    "    except Exception:\n",
    "        pass\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if not GOOGLE_MAPS_API_KEY:\n",
    "    raise ValueError(\n",
    "        \"No Google Maps API key found. Set env var GOOGLE_MAPS_API_KEY or `%store google_maps_API_Key`.\"\n",
    "    )\n",
    "\n",
    "GMAPS_CLIENT = googlemaps.Client(key=GOOGLE_MAPS_API_KEY)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Settings\n",
    "# -----------------------------------------------------------------------------\n",
    "INPUT_MD   = \"SoFlaRecordsScraper - MiamiDade.csv\"\n",
    "INPUT_PBC  = \"SoFlaRecordsScraper - PalmBeachCounty.csv\"\n",
    "INPUT_BRD  = \"SoFlaRecordsScraper - Broward.csv\"\n",
    "OUTPUT_GEOJSON = \"map_data.geojson\"\n",
    "GEO_CACHE_PATH = \"geocode_cache.json\"\n",
    "DEFAULT_CRS = \"EPSG:4326\"\n",
    "\n",
    "TEXT_TO_DROP = \"Scrape Attempt Date:\"  # sentinel rows to drop\n",
    "COLUMNS_TO_KEEP = [  # used when present; we won’t fail if some are missing\n",
    "    \"ScrapeDate\",\"Doc Type\",\"Instrument_Num\",\"Record Date\",\"Record Date Search\",\n",
    "    \"Seller\",\"Buyer\",\"Consideration\",\"Folio\",\"Use Code Description\",\"Building Sq. Ft\",\n",
    "    \"Lot Size\",\"Date of Previous Sale\",\"Previous Owner Name\",\"Previous Sale Price\",\n",
    "    \"Physical Address\",\"Mailing Address\",\"Municipality\",\"PropAppraiserURL\",\n",
    "    \"Sunbiz Doc URL First Party\",\"Sunbiz Doc URL Second Party\",\n",
    "    \"First Party Registered Agent Name & Address\",\"First Party Document Number\",\n",
    "    \"First Party FEI/EIN Number\",\"First Party Mailing Address\",\n",
    "    \"First Party Principal Address\",\"First Party State\",\"First Party Date Filed\",\n",
    "    \"Second Party Registered Agent Name & Address\",\"Second Party Status\",\n",
    "    \"Second Party Document Number\",\"Second Party FEI/EIN Number\",\n",
    "    \"Second Party Mailing Address\",\"Second Party Principal Address\",\n",
    "    \"Second Party State\",\"Second Party Date Filed\",\n",
    "]\n",
    "\n",
    "RES_CLASSES = [\"RESIDENTIAL\",\"CONDOMINIUM\",\"CONDO\",\"FAMILY\",\"RV PARK\"]\n",
    "COM_CLASSES = [\n",
    "    \"OFFICE\",\"MANUFACTURING\",\"COMMERCIAL\",\"HOTEL\",\"MOTEL\",\"INDUSTRIAL\",\"HEAVY IND\",\n",
    "    \"GOLF COURSE\",\"RETAIL\",\"WAREH/DIST TERM\",\"WAREHOUSE\",\"STORAGE\",\"MULTIFAMILY\",\n",
    "    \"SCHOOL\",\"RESTAURANTS\",\"SHOPPING CENTER\",\"MULTI-FAMILY\",\"SERVICE STATION\",\n",
    "    \"DRUG STORE\",\"RELIGIOUS\",\"WAREHOUSING\",\"NIGHTCLUBS\",\"PARKING LOT\",\n",
    "]\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "log = logging.getLogger(\"transactions-geojson\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helpers\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def _drop_rows_with_text(df: pd.DataFrame, text: str) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "    mask_keep = ~df.astype(str).apply(\n",
    "        lambda col: col.str.contains(text, na=False)\n",
    "    ).any(axis=1)\n",
    "    return df.loc[mask_keep]\n",
    "\n",
    "\n",
    "def format_md_folio(folio: str) -> str:\n",
    "    s = str(folio)\n",
    "    return f\"{s[0:2]}-{s[2:6]}-{s[6:9]}-{s[9:13]}\" if (s.isdigit() and len(s) == 13) else s\n",
    "\n",
    "\n",
    "def extract_broward_folio(url: str) -> Optional[str]:\n",
    "    if isinstance(url, str) and \"Folio=\" in url:\n",
    "        return url.split(\"Folio=\")[1]\n",
    "    return None\n",
    "\n",
    "\n",
    "def make_anchor(text: str, url: Optional[str]) -> str:\n",
    "    if pd.isna(url) or url in {\"No matching document found\",\"No Prop Appraiser URL Found\",\"Data Not Found\"}:\n",
    "        return str(text)\n",
    "    return f'<a href=\"{url}\" target=\"_blank\">{text}</a>'\n",
    "\n",
    "\n",
    "def normalize_pbc_addresses(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for col in [\"Physical Address\",\"Municipality\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.strip().replace({\"Data Not Found\": \"\"}).fillna(\"\")\n",
    "    has_addr = df.get(\"Physical Address\", \"\") != \"\"\n",
    "    is_uninc = df.get(\"Municipality\", \"\").eq(\"UNINCORPORATED\") if \"Municipality\" in df.columns else False\n",
    "    df.loc[has_addr & is_uninc, \"Physical Address\"] = (\n",
    "        df.loc[has_addr & is_uninc, \"Physical Address\"] + \" IN UNINCORPORATED PALM BEACH COUNTY\"\n",
    "    )\n",
    "    df.loc[has_addr & ~is_uninc, \"Physical Address\"] = (\n",
    "        df.loc[has_addr & ~is_uninc, \"Physical Address\"].str.cat(\n",
    "            df.loc[has_addr & ~is_uninc, \"Municipality\"], sep=\" IN \"\n",
    "        )\n",
    "    )\n",
    "    if \"Municipality\" in df.columns:\n",
    "        df = df.drop(columns=\"Municipality\")\n",
    "    df[\"Physical Address\"] = df[\"Physical Address\"].str.replace(\n",
    "        r\" in $\", \"Data Not Found\", regex=True\n",
    "    ).str.strip(\", \")\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_zip_from_address(df: pd.DataFrame, addr_col: str) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    z = df[addr_col].astype(str).str.extract(r\"(\\b\\d{5}(?:-\\d{4})?\\b)\", expand=False)\n",
    "    df[\"ZipCode\"] = z\n",
    "    df[addr_col] = df[addr_col].astype(str).str.replace(\n",
    "        r\"\\b\\d{5}(?:-\\d{4})?\\b\", \"\", regex=True\n",
    "    ).str.strip().str.strip(\",\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def classify_use_code(series: pd.Series) -> pd.Series:\n",
    "    s = series.fillna(\"\").astype(str).str.upper()\n",
    "    res_pat = \"|\".join(map(re.escape, RES_CLASSES))\n",
    "    com_pat = \"|\".join(map(re.escape, COM_CLASSES))\n",
    "    out = pd.Series(\"OTHER\", index=s.index)\n",
    "    out[s.str.contains(res_pat, na=False)] = \"RESIDENTIAL\"\n",
    "    out[s.str.contains(com_pat, na=False)] = \"COMMERCIAL\"\n",
    "    return out\n",
    "\n",
    "\n",
    "def normalize_doc_types(s: pd.Series) -> pd.Series:\n",
    "    s = s.fillna(\"\").astype(str)\n",
    "    replacements = {\n",
    "        r\"\\bDEE\\b\": \"DEED\",\n",
    "        r\"\\bDeed Transfers of Real Property\\b\": \"DEED\",\n",
    "        r\"\\bMOR\\b\": \"MORTGAGE\",\n",
    "        r\"Mortgage/ Modifications & Assumptions\": \"MORTGAGE\",\n",
    "    }\n",
    "    for pat, repl in replacements.items():\n",
    "        s = s.str.replace(pat, repl, regex=True)\n",
    "    return s\n",
    "\n",
    "\n",
    "def clean_sale_price(s: pd.Series) -> pd.Series:\n",
    "    s = s.replace([\"None\",\"No price found!\",\"Data Not Found\",\"No results.\"], np.nan)\n",
    "    s = s.astype(str).str.replace(\"$\", \"\", regex=False).str.replace(\",\", \"\", regex=False)\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "\n",
    "def add_anchors(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if {\"Sunbiz Doc URL First Party\",\"Seller\"}.issubset(df.columns):\n",
    "        df[\"Seller\"] = [make_anchor(t, u) for t, u in zip(df[\"Seller\"].astype(str), df[\"Sunbiz Doc URL First Party\"])]\n",
    "    if {\"Sunbiz Doc URL Second Party\",\"Buyer\"}.issubset(df.columns):\n",
    "        df[\"Buyer\"] = [make_anchor(t, u) for t, u in zip(df[\"Buyer\"].astype(str), df[\"Sunbiz Doc URL Second Party\"])]\n",
    "    if {\"PropAppraiserURL\",\"Folio\"}.issubset(df.columns):\n",
    "        df[\"Folio\"] = [make_anchor(str(t), u) for t, u in zip(df[\"Folio\"], df[\"PropAppraiserURL\"])]\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Geocoding (cache + normalization + FL/US bias)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def load_cache(path: str) -> Dict[str, Tuple[float, float]]:\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                return json.load(f)\n",
    "        except Exception:\n",
    "            return {}\n",
    "    return {}\n",
    "\n",
    "\n",
    "def save_cache(path: str, cache: Dict[str, Tuple[float, float]]) -> None:\n",
    "    tmp = path + \".tmp\"\n",
    "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(cache, f)\n",
    "    os.replace(tmp, path)\n",
    "\n",
    "county_suffix_re = re.compile(r\"\\s+(MIAMI-DADE|PALM BEACH COUNTY|BROWARD COUNTY)\\b\", re.I)\n",
    "zip4_zeros_re    = re.compile(r\"(\\b\\d{5})-0000\\b\")\n",
    "zipcode_re       = re.compile(r\"\\b\\d{5}(?:-\\d{4})?\\b\")\n",
    "\n",
    "def normalize_for_geocode(a: str) -> str:\n",
    "    if not isinstance(a, str):\n",
    "        return \"\"\n",
    "    s = a.strip()\n",
    "    s = county_suffix_re.sub(\"\", s)                               # drop appended county\n",
    "    s = re.sub(r\",?\\s*UNINCORPORATED COUNTY\\b\", \"\", s, flags=re.I)\n",
    "    s = zip4_zeros_re.sub(r\"\\1\", s)                               # 33178-0000 -> 33178\n",
    "    s = re.sub(r\"\\s{2,}\", \" \", s)\n",
    "    s = re.sub(r\",\\s*,\", \", \", s)\n",
    "    return s.strip(\", \").strip()\n",
    "\n",
    "\n",
    "def geocode_addresses(\n",
    "    df: pd.DataFrame,\n",
    "    addr_col: str,\n",
    "    cache_path: str,\n",
    "    gmaps_client: Optional[googlemaps.Client],\n",
    "    show_progress: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    cache: Dict[str, Tuple[float, float]] = load_cache(cache_path)\n",
    "    gmaps = gmaps_client\n",
    "\n",
    "    def _geo_one(addr: str) -> Tuple[Optional[float], Optional[float]]:\n",
    "        if not addr:\n",
    "            return (None, None)\n",
    "        if addr in cache:\n",
    "            return cache[addr]\n",
    "        if gmaps is None:\n",
    "            return (None, None)\n",
    "        try:\n",
    "            # Primary attempt with FL/US bias\n",
    "            result = gmaps.geocode(\n",
    "                addr,\n",
    "                components={\"administrative_area\": \"FL\", \"country\": \"US\"},\n",
    "                region=\"us\",\n",
    "            )\n",
    "            # Fallback: drop zip entirely\n",
    "            if not result:\n",
    "                no_zip = zipcode_re.sub(\"\", addr).replace(\"  \", \" \").strip(\", \")\n",
    "                if no_zip:\n",
    "                    result = gmaps.geocode(\n",
    "                        f\"{no_zip}, FL, USA\",\n",
    "                        components={\"administrative_area\": \"FL\", \"country\": \"US\"},\n",
    "                        region=\"us\",\n",
    "                    )\n",
    "            if result:\n",
    "                lat = result[0][\"geometry\"][\"location\"][\"lat\"]\n",
    "                lng = result[0][\"geometry\"][\"location\"][\"lng\"]\n",
    "                cache[addr] = (lat, lng)\n",
    "                return (lat, lng)\n",
    "        except Exception as e:\n",
    "            log.warning(\"Geocode failed for '%s': %s\", addr, e)\n",
    "        return (None, None)\n",
    "\n",
    "    norm = df[addr_col].fillna(\"\").astype(str).map(normalize_for_geocode)\n",
    "    unique_addrs = norm.unique().tolist()\n",
    "    log.info(\"Unique addresses to resolve (normalized): %d\", len(unique_addrs))\n",
    "\n",
    "    iterator = tqdm(unique_addrs, desc=\"Geocoding\", unit=\"addr\") if show_progress else unique_addrs\n",
    "    for a in iterator:\n",
    "        if a and a not in cache:\n",
    "            _geo_one(a)\n",
    "    save_cache(cache_path, cache)\n",
    "\n",
    "    coords = norm.map(lambda a: cache.get(a, (None, None)))\n",
    "    df[\"latitude\"] = coords.map(lambda t: t[0])\n",
    "    df[\"longitude\"] = coords.map(lambda t: t[1])\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Main\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def main() -> None:\n",
    "    # Remove old geojson if present (optional, harmless if missing)\n",
    "    if os.path.exists(OUTPUT_GEOJSON):\n",
    "        os.remove(OUTPUT_GEOJSON)\n",
    "        log.info(\"Deleted old %s\", OUTPUT_GEOJSON)\n",
    "\n",
    "    # 1) Load CSVs (full sheets)\n",
    "    md_df  = pd.read_csv(INPUT_MD, encoding=\"utf-8-sig\")\n",
    "    pbc_df = pd.read_csv(INPUT_PBC, encoding=\"utf-8-sig\")\n",
    "    brd_df = pd.read_csv(INPUT_BRD, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # Keep only columns we recognize (don’t error if others are present/missing)\n",
    "    def _keep(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        avail = [c for c in COLUMNS_TO_KEEP if c in df.columns]\n",
    "        return df[avail].copy() if avail else df.copy()\n",
    "\n",
    "    md_df, pbc_df, brd_df = _keep(md_df), _keep(pbc_df), _keep(brd_df)\n",
    "\n",
    "    # Drop any “scrape attempt” rows (content rows, not transactions)\n",
    "    md_df  = _drop_rows_with_text(md_df,  TEXT_TO_DROP)\n",
    "    pbc_df = _drop_rows_with_text(pbc_df, TEXT_TO_DROP)\n",
    "    brd_df = _drop_rows_with_text(brd_df, TEXT_TO_DROP)\n",
    "\n",
    "    # 2) County-specific tweaks\n",
    "    if \"Folio\" in md_df.columns:\n",
    "        md_df[\"Folio\"] = md_df[\"Folio\"].astype(str).map(format_md_folio)\n",
    "    if \"PropAppraiserURL\" in brd_df.columns:\n",
    "        brd_df[\"Folio\"] = brd_df[\"PropAppraiserURL\"].map(extract_broward_folio)\n",
    "\n",
    "    # Full Address + County (for display only)\n",
    "    if \"Physical Address\" in md_df.columns:\n",
    "        md_df[\"Full Address\"] = md_df[\"Physical Address\"].astype(str) + \" MIAMI-DADE\"\n",
    "        md_df[\"County\"] = \"Miami-Dade\"\n",
    "    if \"Physical Address\" in pbc_df.columns:\n",
    "        pbc_df[\"Full Address\"] = pbc_df[\"Physical Address\"].astype(str) + \" PALM BEACH COUNTY\"\n",
    "        pbc_df[\"County\"] = \"Palm Beach\"\n",
    "    if \"Physical Address\" in brd_df.columns:\n",
    "        brd_df[\"Full Address\"] = brd_df[\"Physical Address\"].astype(str) + \" BROWARD COUNTY\"\n",
    "        brd_df[\"County\"] = \"Broward\"\n",
    "\n",
    "    # PBC: merge municipality into address & drop Municipality column\n",
    "    pbc_df = normalize_pbc_addresses(pbc_df)\n",
    "\n",
    "    # Split ZIP from Physical Address for MD & Broward (optional, keeps ZipCode col)\n",
    "    if \"Physical Address\" in md_df.columns:\n",
    "        md_df  = split_zip_from_address(md_df,  \"Physical Address\")\n",
    "    if \"Physical Address\" in brd_df.columns:\n",
    "        brd_df = split_zip_from_address(brd_df, \"Physical Address\")\n",
    "\n",
    "    # 3) Concatenate — DO NOT drop anything\n",
    "    df = pd.concat([md_df, pbc_df, brd_df], ignore_index=True, sort=False)\n",
    "\n",
    "    # 4) Cleaning / normalization — DO NOT drop anything\n",
    "    if \"Consideration\" in df.columns:\n",
    "        df = df.rename(columns={\"Consideration\": \"Sale Price\"})\n",
    "        df[\"Sale Price\"] = clean_sale_price(df[\"Sale Price\"])  # float\n",
    "\n",
    "    if \"Physical Address\" in df.columns:\n",
    "        df[\"Physical Address\"] = df[\"Physical Address\"].astype(str).str.upper()\n",
    "    if \"Use Code Description\" in df.columns:\n",
    "        df[\"Use Code Description\"] = df[\"Use Code Description\"].astype(str).str.upper()\n",
    "    if \"Doc Type\" in df.columns:\n",
    "        df[\"Doc Type\"] = normalize_doc_types(df[\"Doc Type\"])  # normalize deed/mortgage labels\n",
    "\n",
    "    # Remove \"NOT FOUND\" noise inside cells (keep the row)\n",
    "    df = df.apply(lambda col: col.mask(col.astype(str).str.contains(\"NOT FOUND\", case=False, na=False), \"\"))\n",
    "\n",
    "    # Classification\n",
    "    if \"Use Code Description\" in df.columns:\n",
    "        df[\"Simple Classification\"] = classify_use_code(df[\"Use Code Description\"])\n",
    "\n",
    "    # Anchors (Sunbiz/Appraiser)\n",
    "    df = add_anchors(df)\n",
    "\n",
    "    # 5) Geocoding (keep all rows; geometry can be null)\n",
    "    geocode_input_col = \"Physical Address\" if \"Physical Address\" in df.columns else \"Full Address\"\n",
    "    df = geocode_addresses(df, addr_col=geocode_input_col, cache_path=GEO_CACHE_PATH, gmaps_client=GMAPS_CLIENT, show_progress=True)\n",
    "\n",
    "    # 6) Bundle mortgages with deeds (enrich deeds) — KEEP ALL ROWS\n",
    "    if {\"Doc Type\",\"Seller\",\"Buyer\",\"Record Date\"}.issubset(df.columns):\n",
    "        deeds = df[df[\"Doc Type\"].eq(\"DEED\")].copy()\n",
    "        mortgages = df[df[\"Doc Type\"].eq(\"MORTGAGE\")].copy()\n",
    "        if not deeds.empty and not mortgages.empty:\n",
    "            mortgages[\"TransactionID\"] = mortgages[\"Seller\"].astype(str) + mortgages[\"Record Date\"].astype(str)\n",
    "            deeds[\"TransactionID\"] = deeds[\"Buyer\"].astype(str) + deeds[\"Record Date\"].astype(str)\n",
    "            if \"Buyer\" in mortgages.columns:\n",
    "                mortgages = mortgages.rename(columns={\"Buyer\": \"Lender\"})\n",
    "            if \"Sale Price\" in mortgages.columns:\n",
    "                mortgages = mortgages.rename(columns={\"Sale Price\": \"Loan Amount\"})\n",
    "            # Enrich deeds with Lender/Loan Amount; keep mortgages as-is\n",
    "            deeds = deeds.merge(\n",
    "                mortgages[[\"TransactionID\",\"Lender\",\"Loan Amount\"]],\n",
    "                on=\"TransactionID\",\n",
    "                how=\"left\",\n",
    "            )\n",
    "            # Put enriched deeds back with all other rows (mortgages & others)\n",
    "            df = pd.concat([deeds, df[~df[\"Doc Type\"].eq(\"DEED\")]], ignore_index=True, sort=False)\n",
    "\n",
    "    # 6.5) Export ungeocoded rows for cleanup (but keep them in GeoJSON)\n",
    "    if {\"latitude\",\"longitude\"}.issubset(df.columns):\n",
    "        ungeocoded_mask = df[[\"latitude\",\"longitude\"]].isna().any(axis=1)\n",
    "        try:\n",
    "            df.loc[ungeocoded_mask].to_csv(\"ungeocoded.csv\", index=False)\n",
    "            log.info(\"Wrote %d ungeocoded rows to ungeocoded.csv\", int(ungeocoded_mask.sum()))\n",
    "        except Exception as e:\n",
    "            log.warning(\"Could not write ungeocoded.csv: %s\", e)\n",
    "\n",
    "    # 7) GeoDataFrame (geometry can be None)\n",
    "    geometry = [\n",
    "        Point(lon, lat) if pd.notna(lat) and pd.notna(lon) else None\n",
    "        for lon, lat in zip(df.get(\"longitude\"), df.get(\"latitude\"))\n",
    "    ]\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=geometry, crs=DEFAULT_CRS)\n",
    "\n",
    "    # Fill NaNs for readability (properties only; geometry stays None if ungeocoded)\n",
    "    for col in gdf.columns:\n",
    "        if col == \"geometry\":\n",
    "            continue\n",
    "        if pd.api.types.is_numeric_dtype(gdf[col]):\n",
    "            gdf[col] = gdf[col].fillna(0)\n",
    "        else:\n",
    "            gdf[col] = gdf[col].fillna(\"Data Not Found\")\n",
    "\n",
    "    # 8) Write GeoJSON\n",
    "    gdf.to_file(OUTPUT_GEOJSON, driver=\"GeoJSON\")\n",
    "    log.info(\"Wrote %d rows to %s (geometries may be null)\", len(gdf), OUTPUT_GEOJSON)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
