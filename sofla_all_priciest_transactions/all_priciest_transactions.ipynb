{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52712a48-417d-4e40-837f-0674b576dd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 02:43:32 | INFO | API queries_quota: 60\n",
      "2025-09-25 02:43:32 | INFO | Unique addresses to resolve: 2683\n",
      "Geocoding: 100%|██████████████████████████| 2683/2683 [07:09<00:00,  6.25addr/s]\n",
      "2025-09-25 02:50:42 | INFO | Wrote 2310 ungeocoded rows to ungeocoded.csv\n",
      "2025-09-25 02:50:43 | INFO | Created 5,742 records\n",
      "2025-09-25 02:50:43 | INFO | Wrote 5742 rows to map_data.geojson (3430 geocoded, 2312 ungeocoded)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "from typing import Dict, Optional, Tuple\n",
    "\n",
    "import geopandas as gpd\n",
    "import googlemaps\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------------- Config ----------------------\n",
    "INPUT_MD   = \"SoFlaRecordsScraper - MiamiDade.csv\"\n",
    "INPUT_PBC  = \"SoFlaRecordsScraper - PalmBeachCounty.csv\"\n",
    "INPUT_BRD  = \"SoFlaRecordsScraper - Broward.csv\"\n",
    "\n",
    "OUTPUT_GEOJSON  = \"map_data.geojson\"\n",
    "UNGEOCODED_CSV  = \"ungeocoded.csv\"\n",
    "GEO_CACHE_PATH  = \"geocode_cache.json\"\n",
    "DEFAULT_CRS     = \"EPSG:4326\"\n",
    "\n",
    "# ---- Load Google Maps API key: %store first, env fallback ----\n",
    "try:\n",
    "    get_ipython().run_line_magic(\"store\", \"-r google_maps_API_Key\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if \"google_maps_API_Key\" not in globals() or not google_maps_API_Key:\n",
    "    google_maps_API_Key = os.getenv(\"GOOGLE_MAPS_API_KEY\", \"\")\n",
    "\n",
    "if not google_maps_API_Key:\n",
    "    raise ValueError(\"No Google Maps API key found. Set `%store google_maps_API_Key` or env var GOOGLE_MAPS_API_KEY.\")\n",
    "\n",
    "# Columns we care about (we won’t fail if some are missing)\n",
    "COLUMNS_TO_KEEP = [\n",
    "    \"ScrapeDate\",\"Doc Type\",\"Instrument_Num\",\"Record Date\",\"Record Date Search\",\n",
    "    \"Seller\",\"Buyer\",\"Consideration\",\"Folio\",\"Use Code Description\",\"Building Sq. Ft\",\n",
    "    \"Lot Size\",\"Date of Previous Sale\",\"Previous Owner Name\",\"Previous Sale Price\",\n",
    "    \"Physical Address\",\"Mailing Address\",\"Municipality\",\"PropAppraiserURL\",\n",
    "    \"Sunbiz Doc URL First Party\",\"Sunbiz Doc URL Second Party\",\n",
    "    \"First Party Registered Agent Name & Address\",\"First Party Document Number\",\n",
    "    \"First Party FEI/EIN Number\",\"First Party Mailing Address\",\"First Party Principal Address\",\n",
    "    \"First Party State\",\"First Party Date Filed\",\"Second Party Registered Agent Name & Address\",\n",
    "    \"Second Party Status\",\"Second Party Document Number\",\"Second Party FEI/EIN Number\",\n",
    "    \"Second Party Mailing Address\",\"Second Party Principal Address\",\"Second Party State\",\n",
    "    \"Second Party Date Filed\",\n",
    "]\n",
    "\n",
    "RES_CLASSES = [\"RESIDENTIAL\",\"CONDOMINIUM\",\"CONDO\",\"FAMILY\",\"RV PARK\"]\n",
    "COM_CLASSES = [\n",
    "    \"OFFICE\",\"MANUFACTURING\",\"COMMERCIAL\",\"HOTEL\",\"MOTEL\",\"INDUSTRIAL\",\"HEAVY IND\",\n",
    "    \"GOLF COURSE\",\"RETAIL\",\"WAREH/DIST TERM\",\"WAREHOUSE\",\"STORAGE\",\"MULTIFAMILY\",\n",
    "    \"SCHOOL\",\"RESTAURANTS\",\"SHOPPING CENTER\",\"MULTI-FAMILY\",\"SERVICE STATION\",\n",
    "    \"DRUG STORE\",\"RELIGIOUS\",\"WAREHOUSING\",\"NIGHTCLUBS\",\"PARKING LOT\",\n",
    "]\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "log = logging.getLogger(\"sofla-geo\")\n",
    "\n",
    "# ---------------------- Helpers ----------------------\n",
    "def _keep_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    avail = [c for c in COLUMNS_TO_KEEP if c in df.columns]\n",
    "    return df[avail].copy() if avail else df.copy()\n",
    "\n",
    "def format_md_folio(folio: str) -> str:\n",
    "    s = str(folio)\n",
    "    return f\"{s[0:2]}-{s[2:6]}-{s[6:9]}-{s[9:13]}\" if (s.isdigit() and len(s) == 13) else s\n",
    "\n",
    "def extract_broward_folio(url: str) -> Optional[str]:\n",
    "    if isinstance(url, str) and \"Folio=\" in url:\n",
    "        return url.split(\"Folio=\")[1]\n",
    "    return None\n",
    "\n",
    "def make_anchor(text: str, url: Optional[str]) -> str:\n",
    "    if pd.isna(url) or url in {\"No matching document found\",\"No Prop Appraiser URL Found\",\"Data Not Found\"}:\n",
    "        return str(text)\n",
    "    return f'<a href=\"{url}\" target=\"_blank\">{text}</a>'\n",
    "\n",
    "def normalize_pbc_addresses(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for col in [\"Physical Address\",\"Municipality\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.strip().replace({\"Data Not Found\": \"\"}).fillna(\"\")\n",
    "    has_addr = df.get(\"Physical Address\",\"\") != \"\"\n",
    "    is_uninc = df.get(\"Municipality\",\"\").eq(\"UNINCORPORATED\") if \"Municipality\" in df.columns else False\n",
    "    df.loc[has_addr & is_uninc, \"Physical Address\"] = (\n",
    "        df.loc[has_addr & is_uninc, \"Physical Address\"] + \" IN UNINCORPORATED PALM BEACH COUNTY\"\n",
    "    )\n",
    "    df.loc[has_addr & ~is_uninc, \"Physical Address\"] = (\n",
    "        df.loc[has_addr & ~is_uninc, \"Physical Address\"].str.cat(\n",
    "            df.loc[has_addr & ~is_uninc, \"Municipality\"], sep=\" IN \"\n",
    "        )\n",
    "    )\n",
    "    if \"Municipality\" in df.columns:\n",
    "        df = df.drop(columns=\"Municipality\")\n",
    "    df[\"Physical Address\"] = df[\"Physical Address\"].str.replace(r\" in $\",\"Data Not Found\",regex=True).str.strip(\", \")\n",
    "    return df\n",
    "\n",
    "def split_zip_from_address(df: pd.DataFrame, addr_col: str) -> pd.DataFrame:\n",
    "    \"\"\"Capture ZIP to ZipCode column; DO NOT alter any 'Full Address'.\"\"\"\n",
    "    df = df.copy()\n",
    "    z = df[addr_col].astype(str).str.extract(r\"(\\b\\d{5}(?:-\\d{4})?\\b)\", expand=False)\n",
    "    df[\"ZipCode\"] = z\n",
    "    df[addr_col] = df[addr_col].astype(str).str.replace(\n",
    "        r\"\\b\\d{5}(?:-\\d{4})?\\b\",\"\",regex=True\n",
    "    ).str.strip().str.strip(\",\")\n",
    "    return df\n",
    "\n",
    "def classify_use_code(series: pd.Series) -> pd.Series:\n",
    "    s = series.fillna(\"\").astype(str).str.upper()\n",
    "    res_pat = \"|\".join(map(re.escape, RES_CLASSES))\n",
    "    com_pat = \"|\".join(map(re.escape, COM_CLASSES))\n",
    "    out = pd.Series(\"OTHER\", index=s.index)\n",
    "    out[s.str.contains(res_pat, na=False)] = \"RESIDENTIAL\"\n",
    "    out[s.str_contains(com_pat, na=False)] = \"COMMERCIAL\"\n",
    "    return out\n",
    "\n",
    "def classify_use_code(series: pd.Series) -> pd.Series:  # (fixed: older pandas compat)\n",
    "    s = series.fillna(\"\").astype(str).str.upper()\n",
    "    res_pat = \"|\".join(map(re.escape, RES_CLASSES))\n",
    "    com_pat = \"|\".join(map(re.escape, COM_CLASSES))\n",
    "    out = pd.Series(\"OTHER\", index=s.index)\n",
    "    out[s.str.contains(res_pat, na=False)] = \"RESIDENTIAL\"\n",
    "    out[s.str.contains(com_pat, na=False)] = \"COMMERCIAL\"\n",
    "    return out\n",
    "\n",
    "def normalize_doc_types(s: pd.Series) -> pd.Series:\n",
    "    s = s.fillna(\"\").astype(str)\n",
    "    for pat, repl in {\n",
    "        r\"\\bDEE\\b\":\"DEED\",\n",
    "        r\"\\bDeed Transfers of Real Property\\b\":\"DEED\",\n",
    "        r\"\\bMOR\\b\":\"MORTGAGE\",\n",
    "        r\"Mortgage/ Modifications & Assumptions\":\"MORTGAGE\",\n",
    "    }.items():\n",
    "        s = s.str.replace(pat, repl, regex=True)\n",
    "    return s\n",
    "\n",
    "def clean_sale_price(s: pd.Series) -> pd.Series:\n",
    "    s = s.replace([\"None\",\"No price found!\",\"Data Not Found\",\"No results.\"], np.nan)\n",
    "    s = s.astype(str).str.replace(\"$\",\"\",regex=False).str.replace(\",\",\"\",regex=False)\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def add_anchors(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if {\"Sunbiz Doc URL First Party\",\"Seller\"}.issubset(df.columns):\n",
    "        df[\"Seller\"] = [make_anchor(t,u) for t,u in zip(df[\"Seller\"].astype(str), df[\"Sunbiz Doc URL First Party\"])]\n",
    "    if {\"Sunbiz Doc URL Second Party\",\"Buyer\"}.issubset(df.columns):\n",
    "        df[\"Buyer\"] = [make_anchor(t,u) for t,u in zip(df[\"Buyer\"].astype(str), df[\"Sunbiz Doc URL Second Party\"])]\n",
    "    if {\"PropAppraiserURL\",\"Folio\"}.issubset(df.columns):\n",
    "        df[\"Folio\"] = [make_anchor(str(t),u) for t,u in zip(df[\"Folio\"], df[\"PropAppraiserURL\"])]\n",
    "    return df\n",
    "\n",
    "# ---------------------- Geocoding w/ cache ----------------------\n",
    "def load_cache(path: str) -> Dict[str, Tuple[float,float]]:\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                return json.load(f)\n",
    "        except Exception:\n",
    "            return {}\n",
    "    return {}\n",
    "\n",
    "def save_cache(path: str, cache: Dict[str, Tuple[float,float]]) -> None:\n",
    "    tmp = path + \".tmp\"\n",
    "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(cache, f)\n",
    "    os.replace(tmp, path)\n",
    "\n",
    "zipcode_re    = re.compile(r\"\\b\\d{5}(?:-\\d{4})?\\b\")\n",
    "zip4_zeros_re = re.compile(r\"(\\b\\d{5})-0000\\b\")\n",
    "\n",
    "def geocode_addresses(\n",
    "    df: pd.DataFrame,\n",
    "    addr_col: str,\n",
    "    cache_path: str,\n",
    "    api_key: str,\n",
    "    show_progress: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Geocode unique addresses using Google Maps with FL/US bias.\n",
    "       Keep address string (incl. county suffix) as provided.\n",
    "       Fallback: drop ZIP; try '{no_zip}, FL, USA'.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    cache = load_cache(cache_path)\n",
    "    gmaps = googlemaps.Client(key=api_key)\n",
    "\n",
    "    def _geo_one(addr: str) -> Tuple[Optional[float], Optional[float]]:\n",
    "        if not addr:\n",
    "            return (None, None)\n",
    "        if addr in cache:\n",
    "            return cache[addr]\n",
    "        try:\n",
    "            # slight normalization: collapse '-0000' in ZIP9\n",
    "            addr_try = zip4_zeros_re.sub(r\"\\1\", addr).strip()\n",
    "            result = gmaps.geocode(\n",
    "                addr_try,\n",
    "                components={\"administrative_area\": \"FL\", \"country\": \"US\"},\n",
    "                region=\"us\",\n",
    "            )\n",
    "            if not result:\n",
    "                # fallback: drop ZIP entirely and try with explicit FL, USA\n",
    "                no_zip = zipcode_re.sub(\"\", addr_try).replace(\"  \", \" \").strip(\", \").strip()\n",
    "                if no_zip:\n",
    "                    result = gmaps.geocode(\n",
    "                        f\"{no_zip}, FL, USA\",\n",
    "                        components={\"administrative_area\": \"FL\", \"country\": \"US\"},\n",
    "                        region=\"us\",\n",
    "                    )\n",
    "            if result:\n",
    "                lat = result[0][\"geometry\"][\"location\"][\"lat\"]\n",
    "                lng = result[0][\"geometry\"][\"location\"][\"lng\"]\n",
    "                cache[addr] = (lat, lng)  # cache against the original string\n",
    "                return (lat, lng)\n",
    "        except Exception as e:\n",
    "            log.warning(\"Geocode failed for '%s': %s\", addr, e)\n",
    "        return (None, None)\n",
    "\n",
    "    if addr_col not in df.columns:\n",
    "        # Guarantee output columns exist even if addr_col is missing\n",
    "        df[\"latitude\"] = np.nan\n",
    "        df[\"longitude\"] = np.nan\n",
    "        return df\n",
    "\n",
    "    addrs = df[addr_col].fillna(\"\").astype(str)\n",
    "    unique_addrs = addrs.unique().tolist()\n",
    "    log.info(\"Unique addresses to resolve: %d\", len(unique_addrs))\n",
    "\n",
    "    iterator = tqdm(unique_addrs, desc=\"Geocoding\", unit=\"addr\") if show_progress else unique_addrs\n",
    "    for a in iterator:\n",
    "        if a and a not in cache:\n",
    "            _geo_one(a)\n",
    "    save_cache(cache_path, cache)\n",
    "\n",
    "    coords = addrs.map(lambda a: cache.get(a, (None, None)))\n",
    "    df[\"latitude\"]  = coords.map(lambda t: t[0])\n",
    "    df[\"longitude\"] = coords.map(lambda t: t[1])\n",
    "    return df\n",
    "\n",
    "# ---------------------- Main ----------------------\n",
    "def main() -> None:\n",
    "    # Start clean (optional)\n",
    "    if os.path.exists(OUTPUT_GEOJSON):\n",
    "        os.remove(OUTPUT_GEOJSON)\n",
    "        log.info(\"Deleted old %s\", OUTPUT_GEOJSON)\n",
    "\n",
    "    # 1) Load CSVs (full sheets)\n",
    "    md_df  = pd.read_csv(INPUT_MD,  encoding=\"utf-8-sig\")\n",
    "    pbc_df = pd.read_csv(INPUT_PBC, encoding=\"utf-8-sig\")\n",
    "    brd_df = pd.read_csv(INPUT_BRD, encoding=\"utf-8-sig\")\n",
    "\n",
    "    md_df, pbc_df, brd_df = _keep_columns(md_df), _keep_columns(pbc_df), _keep_columns(brd_df)\n",
    "\n",
    "    # 2) County-specific tweaks\n",
    "    if \"Folio\" in md_df.columns:\n",
    "        md_df[\"Folio\"] = md_df[\"Folio\"].astype(str).map(format_md_folio)\n",
    "    if \"PropAppraiserURL\" in brd_df.columns:\n",
    "        brd_df[\"Folio\"] = brd_df[\"PropAppraiserURL\"].map(extract_broward_folio)\n",
    "\n",
    "    # Full Address + County (for geocoding & display) — build BEFORE any ZIP edits\n",
    "    if \"Physical Address\" in md_df.columns:\n",
    "        md_df[\"Full Address\"] = md_df[\"Physical Address\"].astype(str) + \" MIAMI-DADE\"\n",
    "        md_df[\"County\"] = \"Miami-Dade\"\n",
    "    if \"Physical Address\" in pbc_df.columns:\n",
    "        pbc_df[\"Full Address\"] = pbc_df[\"Physical Address\"].astype(str) + \" PALM BEACH COUNTY\"\n",
    "        pbc_df[\"County\"] = \"Palm Beach\"\n",
    "    if \"Physical Address\" in brd_df.columns:\n",
    "        brd_df[\"Full Address\"] = brd_df[\"Physical Address\"].astype(str) + \" BROWARD COUNTY\"\n",
    "        brd_df[\"County\"] = \"Broward\"\n",
    "\n",
    "    # PBC municipality logic\n",
    "    pbc_df = normalize_pbc_addresses(pbc_df)\n",
    "\n",
    "    # Optional ZIP extraction for MD & Broward (does not alter Full Address)\n",
    "    if \"Physical Address\" in md_df.columns:\n",
    "        md_df = split_zip_from_address(md_df, \"Physical Address\")\n",
    "    if \"Physical Address\" in brd_df.columns:\n",
    "        brd_df = split_zip_from_address(brd_df, \"Physical Address\")\n",
    "\n",
    "    # 3) Concatenate (keep everything)\n",
    "    df = pd.concat([md_df, pbc_df, brd_df], ignore_index=True, sort=False)\n",
    "\n",
    "    # 4) Cleaning / normalization (NO row drops for policy safety)\n",
    "    if \"Consideration\" in df.columns:\n",
    "        df = df.rename(columns={\"Consideration\": \"Sale Price\"})\n",
    "        df[\"Sale Price\"] = clean_sale_price(df[\"Sale Price\"])\n",
    "\n",
    "    if \"Doc Type\" in df.columns:\n",
    "        df[\"Doc Type\"] = normalize_doc_types(df[\"Doc Type\"])\n",
    "\n",
    "    if \"Use Code Description\" in df.columns:\n",
    "        df[\"Use Code Description\"] = df[\"Use Code Description\"].astype(str).str.upper()\n",
    "        df[\"Simple Classification\"] = classify_use_code(df[\"Use Code Description\"])\n",
    "\n",
    "    # Replace cells containing NOT FOUND with empty string (keep the row)\n",
    "    df = df.apply(lambda c: c.mask(c.astype(str).str.contains(\"NOT FOUND\", case=False, na=False), \"\"))\n",
    "\n",
    "    # Anchors\n",
    "    df = add_anchors(df)\n",
    "\n",
    "    # 5) Geocoding on Full Address (keep all rows; geometry may be null)\n",
    "    if \"Full Address\" not in df.columns:\n",
    "        df[\"Full Address\"] = df.get(\"Physical Address\",\"\")\n",
    "    df = geocode_addresses(df, addr_col=\"Full Address\", cache_path=GEO_CACHE_PATH, api_key=google_maps_API_Key, show_progress=True)\n",
    "\n",
    "    # Ensure geocode columns exist (defensive)\n",
    "    if \"latitude\" not in df.columns:\n",
    "        df[\"latitude\"] = np.nan\n",
    "    if \"longitude\" not in df.columns:\n",
    "        df[\"longitude\"] = np.nan\n",
    "\n",
    "    # Export ungeocoded rows for triage (they stay in GeoJSON as geometry=null)\n",
    "    miss_mask = df[[\"latitude\",\"longitude\"]].isna().any(axis=1)\n",
    "    try:\n",
    "        df.loc[miss_mask].to_csv(UNGEOCODED_CSV, index=False)\n",
    "        log.info(\"Wrote %d ungeocoded rows to %s\", int(miss_mask.sum()), UNGEOCODED_CSV)\n",
    "    except Exception as e:\n",
    "        log.warning(\"Could not write %s: %s\", UNGEOCODED_CSV, e)\n",
    "\n",
    "    # 6) Bundle mortgages with deeds (enrich deeds) — keep mortgages too\n",
    "    if {\"Doc Type\",\"Seller\",\"Buyer\",\"Record Date\"}.issubset(df.columns):\n",
    "        deeds     = df[df[\"Doc Type\"].eq(\"DEED\")].copy()\n",
    "        mortgages = df[df[\"Doc Type\"].eq(\"MORTGAGE\")].copy()\n",
    "        if not deeds.empty and not mortgages.empty:\n",
    "            mortgages[\"TransactionID\"] = mortgages[\"Seller\"].astype(str) + mortgages[\"Record Date\"].astype(str)\n",
    "            deeds[\"TransactionID\"]     = deeds[\"Buyer\"].astype(str)  + deeds[\"Record Date\"].astype(str)\n",
    "            if \"Buyer\" in mortgages.columns:\n",
    "                mortgages = mortgages.rename(columns={\"Buyer\":\"Lender\"})\n",
    "            if \"Sale Price\" in mortgages.columns:\n",
    "                mortgages = mortgages.rename(columns={\"Sale Price\":\"Loan Amount\"})\n",
    "            deeds_enriched = deeds.merge(\n",
    "                mortgages[[\"TransactionID\",\"Lender\",\"Loan Amount\"]],\n",
    "                on=\"TransactionID\", how=\"left\"\n",
    "            )\n",
    "            # keep enriched deeds + all non-deed rows (including mortgages)\n",
    "            others = df[~df[\"Doc Type\"].eq(\"DEED\")]\n",
    "            df = pd.concat([deeds_enriched, others], ignore_index=True, sort=False)\n",
    "\n",
    "    # 7) GeoDataFrame (geometry can be None)\n",
    "    lons = df[\"longitude\"]\n",
    "    lats = df[\"latitude\"]\n",
    "    geometry = [Point(lon, lat) if pd.notna(lat) and pd.notna(lon) else None for lon, lat in zip(lons, lats)]\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=geometry, crs=DEFAULT_CRS)\n",
    "\n",
    "    # Fill NaNs in properties for readability (leave geometry None for misses)\n",
    "    for col in gdf.columns:\n",
    "        if col == \"geometry\":\n",
    "            continue\n",
    "        if pd.api.types.is_numeric_dtype(gdf[col]):\n",
    "            gdf[col] = gdf[col].fillna(0)\n",
    "        else:\n",
    "            gdf[col] = gdf[col].fillna(\"Data Not Found\")\n",
    "\n",
    "    # 8) Write GeoJSON\n",
    "    gdf.to_file(OUTPUT_GEOJSON, driver=\"GeoJSON\")\n",
    "    total = len(gdf)\n",
    "    geocoded = int(gdf[\"geometry\"].notna().sum())\n",
    "    log.info(\"Wrote %d rows to %s (%d geocoded, %d ungeocoded)\", total, OUTPUT_GEOJSON, geocoded, total - geocoded)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
